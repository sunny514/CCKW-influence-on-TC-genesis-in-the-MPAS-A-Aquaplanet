{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f3215bdd-b21e-4ff2-bd9a-27cc34806e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of this script is to generate a pandas dataframe for all candidate ndv/dv's of feature values calculated at each storm center at some time between the CCKW intersection time and up to X day(s) afterwards.\n",
    "# Feature values are anomalized and plotted. Basic feature statistics are also calculated and explored. \n",
    "\n",
    "# INPUTS\n",
    "# --------------------------------------------------------\n",
    "# ds <- Rosi's aquaplanet dataset \n",
    "# selected_eq_dv_ndv_timeseries.csv <- timeseries of selected developers and nondevelopers for ANN ML analysis, generated by 02_select_storms_and_explore \n",
    "# ndv_time_criteria <- Defined in 02_select_storms_and_explore; how long do ndv's have to live for (in days) after crossing a CCKW crest?\n",
    "# dt_after_intersection <- user defined variable (below). How long (hrs) after valid CCKW cross do we want to extract information at storm center for ANN effort? \n",
    "# grid_size <- user defined variable (below). Size of n x n grid box around storm center to average feature values over. \n",
    "# n_bins = 10 <- number of bins used for \n",
    "\n",
    "\n",
    "#The following two inputs were generated by Calc_derived_vars_and_find_latitude_averages_aquaplanet.ipynb\n",
    "#vor850.nc -> calculates 850hPa vorticity over all dimensions in the aquaplanet simulation. \n",
    "#lat_avg_values_aquaplanet_with_vor850.nc -> find zonal means of aquaplanet variables (including derived vars such as 850hPa vorticity)\n",
    "\n",
    "\n",
    "# OUTPUTS\n",
    "# --------------------------------------------------------\n",
    "# df_for_ANN.csv <- dataframe consisting of all pertinent information for the ANN effort. We identify a point in time for each developer (relative to the CCKW intersection)\n",
    "# and extract feature variables around the storm center at that point in time from the simulation dataset. \n",
    "# feature_list_for_ANN.txt <- dataframe consisiting of select features that will be used for the ANN effort. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9a309556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant packages \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore, wasserstein_distance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "import papermill\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fb4938ba-3528-4492-83ec-7dcc9da421e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which simulation are we gonna pull data from? \n",
    "data_dir =  Path(\"/glade/u/home/sjsharma/CCKW_Project/data_and_scripts_from_3km_simulation\")\n",
    "\n",
    "#Which folder are we going to save the data in? \n",
    "save_dir = Path(\"/glade/u/home/sjsharma/CCKW_Project/CNN/CNN_outputs_3km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "447c21c4-e88e-4f60-87c0-c790d5b6aad3",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#Read in csv generated by 02_select_storms of candidate devs/non-devs. \n",
    "\n",
    "df = pd.read_csv(f'{save_dir}/02_outputs/selected_eq_dv_ndv_timeseries.csv',parse_dates=['valid_time'])\n",
    "#read in txt file of ndv criteria set in 02 (how long do ndv's have to live for (in days) after crossing a CCKW crest?\n",
    "with open(f'{save_dir}/02_outputs/ndv_time_criteria.txt', \"r\") as f:\n",
    "    ndv_time_criteria = float(f.read().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "082d138d-8f3c-409c-b73d-dbd0ad85b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in Rosi's aquaplanet dataset \n",
    "exp_name = 'TC_3km'\n",
    "pth = \"/glade/campaign/mmm/dpm/rberrios/glade_scratch/MPAS_APE/aqua_sstmax10N/%s/\"%exp_name\n",
    "fname = pth+'latlon/diags_gaussian_global_nospinup_r3600x1800.nc'\n",
    "ds = xr.open_dataset(fname)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "452be599-2174-42a2-af98-a9a9ae5a515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many hours after CCKW intersection do we want to take a data snapshot for CNN? \n",
    "dt_after_intersection = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "47719fce-5a12-41b7-9775-00a9e932a8f4",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#Select lat/lon range for CNN fields (centered on storm lat/lon coords at a given point in time).\n",
    "\n",
    "lat_pixels = 64 #number of latitude grid cells \n",
    "lon_pixels = 64 #number of longitude grid cells \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d9e902fb-1668-41ea-ae9c-cdcf85f22c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use simulation grid spacing to set csv title \n",
    "\n",
    "dgrid = round(ds['lat'].isel(lat=1).item()-ds['lat'].isel(lat=0).item(),3)\n",
    "\n",
    "lat_range = lat_pixels*dgrid\n",
    "lon_range = lon_pixels*dgrid \n",
    "\n",
    "append_to_filename_raw = f'_{dt_after_intersection}_hrs_after_CCKW_cross_{exp_name}_simulation_{dgrid}_deg_grid_resolution_lat_{lat_pixels}_pixels_lon_{lon_pixels}_pixels'\n",
    "append_to_filename = append_to_filename_raw.replace(\".\",\"p\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b020a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of all feature variables from Rosi's dataset (and also add vor850 to the list - an important derived quantity not in the dataset)\n",
    "feature_vars = list(ds.data_vars)\n",
    "feature_vars.append('vor850') #append derived quantity (vor850)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "76edbb3b-043e-4287-a621-7b6a73f3d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load key variables averages dataset generated previously \n",
    "avg_ds = xr.open_dataset(f'{data_dir}/lat_avg_values_aquaplanet_with_vor850.nc')\n",
    "\n",
    "#read in vorticity data \n",
    "vor850 = xr.open_dataset(f'{data_dir}/vor850.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "04f79827-3511-4c1b-b994-b1c21c5b5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create groupby obj of eq dv/ndv time series\n",
    "df_groupby = df.groupby(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f77a93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell identifies the point along the storm track on/after the relevant CCKW intersection where data should be collected (some time b/w CCKW intersection and less than X day(s) afterwards). \n",
    "\n",
    "#Because devs/non-devs have multiple CCKW intersections we must ensure we only grab the relevant ones. \n",
    "\n",
    "# For devs, we shall grab the CCKW preceding tropical cyclogenesis (these have already been selected to have the closest CCKW preceding tc-genesis) \n",
    "\n",
    "# For non-devs, they can have multiple valid CCKW crossings but the storm needs to exist for X day(s) (as defined by ndv_time_criteria) after the cross \n",
    "# and must have full data richness (no missing or skipped pts b/w CCKW cross up to time X day(s) afterwards). \n",
    "\n",
    "selected_rows = [] \n",
    "\n",
    "for ID, group in df_groupby:\n",
    "\n",
    "    # let's work on developers first -> find the closest CCKW preceding tcgenesis and find the desired time to extract a snapshot for ANN analysis - X hours after crossing a CCKW) \n",
    "\n",
    "    if group['developer'].iloc[0] == 1: #if storm is a developer \n",
    "        \n",
    "        idx_tcgen = np.where(group['tc_genesis']==1)[0][0]\n",
    "\n",
    "        idx_cckw_crosses = np.where(group['cckw_crest_cross']==1)[0]\n",
    "\n",
    "        select_idx_cckw_crosses = idx_cckw_crosses[idx_cckw_crosses<idx_tcgen] #all cckw crosses prior to tc_genesis \n",
    "\n",
    "        idx_closest_preceding_cckw_cross_to_tcgen = select_idx_cckw_crosses[np.abs(idx_cckw_crosses-idx_tcgen).argmin()]\n",
    "\n",
    "        #get time at this desired cckw cross\n",
    "\n",
    "        cckw_cross_time = group['valid_time'].iloc[idx_closest_preceding_cckw_cross_to_tcgen] \n",
    "\n",
    "        desired_time = cckw_cross_time + pd.Timedelta(hours=dt_after_intersection)\n",
    "\n",
    "        if not (desired_time in group['valid_time'].values):\n",
    "\n",
    "            raise ValueError(f\"Storm ID {ID} does not contain a datapoint at the desired time {desired_time}\")\n",
    "\n",
    "        #get index of desired time \n",
    "\n",
    "        desired_time_idx = np.where(group['valid_time']==desired_time)[0][0]\n",
    "\n",
    "        #if conditions are met, add this point to df \n",
    "        \n",
    "        selected_rows.append(group.iloc[desired_time_idx])\n",
    "\n",
    "    #for non-developers -> let's find every valid CCKW crossing (that has data points up to X day(s) after the CCKW cross without additional cckw crosses in that time) \n",
    "\n",
    "    else: \n",
    "        \n",
    "        idx_cckw_crosses = np.where(group['cckw_crest_cross']==1)[0]\n",
    "\n",
    "        for idx_cckw_cross in idx_cckw_crosses:\n",
    "\n",
    "            #first check to see if that storm has a valid CCKW crossing (has data through the time of the CCKW cross plus X day(s) after) \n",
    "\n",
    "            cckw_cross_time = group['valid_time'].iloc[idx_cckw_cross]\n",
    "\n",
    "            time_window_max = cckw_cross_time + pd.Timedelta(days=ndv_time_criteria)\n",
    "\n",
    "            if not (time_window_max in group['valid_time'].values):\n",
    "                \n",
    "                continue \n",
    "\n",
    "            time_window_max_idx = np.where( group['valid_time']==time_window_max)[0][0]\n",
    "\n",
    "            #make sure no additional cckw crosses in the slice of time from CCKW cross to X day(s) after \n",
    "\n",
    "            cckw_cross_data_slice = group['cckw_crest_cross'].iloc[idx_cckw_cross+1:time_window_max_idx]\n",
    "\n",
    "            if np.any(cckw_cross_data_slice==1): \n",
    "            \n",
    "                continue \n",
    "\n",
    "            desired_time = cckw_cross_time + pd.Timedelta(hours=dt_after_intersection)\n",
    "\n",
    "            if not (desired_time in group['valid_time'].values):\n",
    "\n",
    "                print(f\"Storm ID {ID} does not contain a datapoint at the desired time {desired_time}\")\n",
    "\n",
    "                continue \n",
    "\n",
    "            #get index of desired time \n",
    "\n",
    "            desired_time_idx = np.where(group['valid_time']==desired_time)[0][0]\n",
    "\n",
    "            #if conditions are met, add this point to df \n",
    "        \n",
    "            selected_rows.append(group.iloc[desired_time_idx])  # double brackets!\n",
    "\n",
    "df_for_CNN = pd.DataFrame(selected_rows).reset_index(drop=True)\n",
    "\n",
    "df_for_CNN['index']=df_for_CNN.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d740f360-78e3-487d-89f9-bc6b9b0a494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's also save off the df for CNN\n",
    "\n",
    "df_for_CNN.to_csv(f'{save_dir}/03_outputs/CNN_config_dataframes/df_for_CNN_{append_to_filename}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "72cf8577-cad7-449c-b45a-a025b199d0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>fhr</th>\n",
       "      <th>valid_time</th>\n",
       "      <th>lon_TRACK</th>\n",
       "      <th>lat_TRACK</th>\n",
       "      <th>vor850</th>\n",
       "      <th>day_adj</th>\n",
       "      <th>tc_genesis</th>\n",
       "      <th>lon_track_hov_idx</th>\n",
       "      <th>day_track_hov_idx</th>\n",
       "      <th>cckw_crest_cross</th>\n",
       "      <th>suspect_point</th>\n",
       "      <th>developer</th>\n",
       "      <th>cckw_filteredpr_val</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>144.0</td>\n",
       "      <td>2000-04-05 00:00:00</td>\n",
       "      <td>11.652895</td>\n",
       "      <td>18.528280</td>\n",
       "      <td>1.493700</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.058350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>246.0</td>\n",
       "      <td>2000-04-09 06:00:00</td>\n",
       "      <td>358.736023</td>\n",
       "      <td>14.347824</td>\n",
       "      <td>1.518751</td>\n",
       "      <td>10.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3587.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.045556</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2000-04-02 12:00:00</td>\n",
       "      <td>346.038757</td>\n",
       "      <td>12.146024</td>\n",
       "      <td>2.358891</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3460.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.037982</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139</td>\n",
       "      <td>114.0</td>\n",
       "      <td>2000-04-03 18:00:00</td>\n",
       "      <td>278.686096</td>\n",
       "      <td>7.357641</td>\n",
       "      <td>2.415950</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2787.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.342577</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>320</td>\n",
       "      <td>204.0</td>\n",
       "      <td>2000-04-07 12:00:00</td>\n",
       "      <td>129.846588</td>\n",
       "      <td>10.095192</td>\n",
       "      <td>2.404459</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225677</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>6279</td>\n",
       "      <td>2994.0</td>\n",
       "      <td>2000-08-01 18:00:00</td>\n",
       "      <td>91.697334</td>\n",
       "      <td>4.613536</td>\n",
       "      <td>1.294153</td>\n",
       "      <td>125.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>917.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.361769</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>6372</td>\n",
       "      <td>3054.0</td>\n",
       "      <td>2000-08-04 06:00:00</td>\n",
       "      <td>119.896683</td>\n",
       "      <td>6.640931</td>\n",
       "      <td>1.116741</td>\n",
       "      <td>127.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1199.0</td>\n",
       "      <td>511.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.011430</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>6471</td>\n",
       "      <td>3102.0</td>\n",
       "      <td>2000-08-06 06:00:00</td>\n",
       "      <td>133.723282</td>\n",
       "      <td>8.651182</td>\n",
       "      <td>1.112677</td>\n",
       "      <td>129.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1337.0</td>\n",
       "      <td>519.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.520501</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>6486</td>\n",
       "      <td>3138.0</td>\n",
       "      <td>2000-08-07 18:00:00</td>\n",
       "      <td>248.875549</td>\n",
       "      <td>4.874249</td>\n",
       "      <td>1.213878</td>\n",
       "      <td>131.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2489.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.412700</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>6500</td>\n",
       "      <td>3144.0</td>\n",
       "      <td>2000-08-08 00:00:00</td>\n",
       "      <td>251.850006</td>\n",
       "      <td>19.979794</td>\n",
       "      <td>2.014773</td>\n",
       "      <td>131.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2519.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.133008</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID     fhr          valid_time   lon_TRACK  lat_TRACK    vor850  \\\n",
       "0       5   144.0 2000-04-05 00:00:00   11.652895  18.528280  1.493700   \n",
       "1       5   246.0 2000-04-09 06:00:00  358.736023  14.347824  1.518751   \n",
       "2      81    84.0 2000-04-02 12:00:00  346.038757  12.146024  2.358891   \n",
       "3     139   114.0 2000-04-03 18:00:00  278.686096   7.357641  2.415950   \n",
       "4     320   204.0 2000-04-07 12:00:00  129.846588  10.095192  2.404459   \n",
       "..    ...     ...                 ...         ...        ...       ...   \n",
       "226  6279  2994.0 2000-08-01 18:00:00   91.697334   4.613536  1.294153   \n",
       "227  6372  3054.0 2000-08-04 06:00:00  119.896683   6.640931  1.116741   \n",
       "228  6471  3102.0 2000-08-06 06:00:00  133.723282   8.651182  1.112677   \n",
       "229  6486  3138.0 2000-08-07 18:00:00  248.875549   4.874249  1.213878   \n",
       "230  6500  3144.0 2000-08-08 00:00:00  251.850006  19.979794  2.014773   \n",
       "\n",
       "     day_adj  tc_genesis  lon_track_hov_idx  day_track_hov_idx  \\\n",
       "0       6.50         0.0              117.0               26.0   \n",
       "1      10.75         0.0             3587.0               43.0   \n",
       "2       4.00         0.0             3460.0               16.0   \n",
       "3       5.25         0.0             2787.0               21.0   \n",
       "4       9.00         0.0             1298.0               36.0   \n",
       "..       ...         ...                ...                ...   \n",
       "226   125.25         0.0              917.0              501.0   \n",
       "227   127.75         0.0             1199.0              511.0   \n",
       "228   129.75         0.0             1337.0              519.0   \n",
       "229   131.25         0.0             2489.0              525.0   \n",
       "230   131.50         0.0             2519.0              526.0   \n",
       "\n",
       "     cckw_crest_cross  suspect_point  developer  cckw_filteredpr_val  index  \n",
       "0                 1.0            0.0          0            -0.058350      0  \n",
       "1                 1.0            0.0          0            -0.045556      1  \n",
       "2                 1.0            0.0          0            -0.037982      2  \n",
       "3                 1.0            0.0          1             0.342577      3  \n",
       "4                 1.0            0.0          0             0.225677      4  \n",
       "..                ...            ...        ...                  ...    ...  \n",
       "226               1.0            0.0          0             0.361769    226  \n",
       "227               1.0            0.0          1             1.011430    227  \n",
       "228               1.0            0.0          1             0.520501    228  \n",
       "229               1.0            0.0          0             0.412700    229  \n",
       "230               1.0            0.0          0             0.133008    230  \n",
       "\n",
       "[231 rows x 15 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6cd9c791-4191-435c-a2aa-45971989b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = f\"{save_dir}/03_outputs/netcdf_files/{append_to_filename[1:]}\"\n",
    "\n",
    "grid_spacing = (np.round(ds.lat[1].item() - ds.lat[0].item(),1))\n",
    "\n",
    "ds_list = [] \n",
    "\n",
    "# build the folder path (without the filename)\n",
    "out_dir = f\"{save_dir}/03_outputs/netcdf_files/{append_to_filename[1:]}\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "for idx, row in df_for_CNN.iterrows():\n",
    "    time = row['valid_time']\n",
    "    lon_center = row['lon_TRACK']\n",
    "    lat_center = row['lat_TRACK']\n",
    "\n",
    "    lat_center_idx = ds.indexes['lat'].get_indexer([lat_center], method=\"nearest\")[0]\n",
    "    lon_center_idx = ds.indexes['lon'].get_indexer([lon_center], method=\"nearest\")[0]\n",
    "\n",
    "    lon_min_idx = lon_center_idx - lon_range/(2*grid_spacing)\n",
    "    lon_max_idx = lon_center_idx + lon_range/(2*grid_spacing) - 1\n",
    "                    \n",
    "    lon_indices = np.linspace(int(lon_min_idx),int(lon_max_idx),int(lon_max_idx-lon_min_idx)+1)\n",
    "    fixed_lon_indices = (lon_indices % 3600).astype(int) #deal w/ wrap-around using modulo operator\n",
    "    time_idx = ds.indexes['time'].get_indexer([time], method=\"nearest\")[0]\n",
    "    \n",
    "    lat_box_size = int(lat_range/grid_spacing)\n",
    "    lon_box_size = int(lon_range/grid_spacing)\n",
    "    \n",
    "    sub_ds_main_vars = ds.isel(\n",
    "    time=time_idx,\n",
    "    lat=slice(lat_center_idx - int(lat_box_size/2), lat_center_idx + int(lat_box_size/2)),\n",
    "    lon=fixed_lon_indices,\n",
    "    )\n",
    "\n",
    "    sub_ds_vor850 = vor850.isel(\n",
    "    time=time_idx,\n",
    "    lat=slice(lat_center_idx - int(lat_box_size/2), lat_center_idx + int(lat_box_size/2)),\n",
    "    lon=fixed_lon_indices,\n",
    "    )\n",
    "\n",
    "    sub_ds = xr.merge([sub_ds_main_vars, sub_ds_vor850])\n",
    "    # ds_list.append(sub_ds)\n",
    "    sub_ds.to_netcdf(f'{out_dir}/ds{append_to_filename}_df_index_{idx}.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c75c71c2-75a2-439d-a329-06e7985dcce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's load up the netcdf datasets you just generated and then put them in an ordered list by index! \n",
    "\n",
    "nc_files = glob.glob(f\"{out_dir}/*.nc\")\n",
    "\n",
    "nc_file_list_ordered = [0]*len(nc_files)\n",
    "\n",
    "for nc_file in nc_files:\n",
    "    \n",
    "    ds = xr.open_dataset(nc_file)\n",
    "\n",
    "    index_raw = str.split(nc_file,'_')[-1]\n",
    "    index = str.split(index_raw,'.')[0]\n",
    "    index_number = int(index)\n",
    "\n",
    "    nc_file_list_ordered[index_number] = ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1e413270-72a8-4482-bcc0-3f6dcf9c9633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(231, 64, 64, 22)\n"
     ]
    }
   ],
   "source": [
    "all_patches = []\n",
    "\n",
    "for ds in nc_file_list_ordered:\n",
    "\n",
    "    # make sure variables are stacked along \"channel\"\n",
    "    arr = ds.to_array(\"channel\").transpose(\"lat\", \"lon\", \"channel\").values\n",
    "    # arr.shape should be (150, 150, 22)\n",
    "\n",
    "    all_patches.append(arr)\n",
    "\n",
    "# stack into one big NumPy array\n",
    "X = np.stack(all_patches, axis=0)\n",
    "print(X.shape)  # (N, 150, 150, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a9728713-9f94-46de-9740-7e9e12e5b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"{save_dir}/03_outputs/numpy_arrays_for_CNN/feature_data{append_to_filename}.npy\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa8731-fdcc-449c-8711-377858272e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Pangeo + PyNGL)",
   "language": "python",
   "name": "my_pangeo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
